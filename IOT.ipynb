{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dafdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install firebase-admin joblib python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# If using Google Colab, you might need to upload your .env file\n",
    "from google.colab import files\n",
    "try:\n",
    "    uploaded = files.upload() # Upload .env file\n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(uploaded[list(uploaded.keys())[0]].decode('utf8'))\n",
    "    print(\".env file uploaded successfully\")\n",
    "except:\n",
    "    print(\"No .env file uploaded or already exists\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Rest of your imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Use environment variable for dataset path\n",
    "data_path = os.getenv('DATASET_PATH', \"/content/drive/MyDrive/datasets - datasets.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()\n",
    "\n",
    "df['CropType'].value_counts()\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df.duplicated().sum()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_columns = ['CropType']\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()  # Save encoder for each column\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])  # Encode directly on the column\n",
    "\n",
    "    # Print mapping of categories to numeric values\n",
    "    print(f\"Mapping for column {column}:\")\n",
    "    for class_, value in zip(label_encoders[column].classes_, range(len(label_encoders[column].classes_))):\n",
    "        print(f\"  {class_} -> {value}\")\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = df[['SoilMoisture', 'temperature', 'Humidity','CropDays']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Heatmap of Correlations\")\n",
    "plt.show()\n",
    "df_encoded = pd.get_dummies(df, columns=['CropType'], drop_first=True)\n",
    "correlation_matrix = df_encoded.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.show()\n",
    "\n",
    "df=df.drop(columns=['CropDays'])\n",
    "\n",
    "df\n",
    "\n",
    "# Splitting data\n",
    "X = df.drop(columns=['Irrigation'])\n",
    "y = df['Irrigation']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data into training and testing sets (only do this once)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the optimized Decision Tree model\n",
    "model = DecisionTreeClassifier(criterion='entropy', class_weight='balanced', max_depth=5,\n",
    "                              min_samples_split=10, min_samples_leaf=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Decision Tree Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(f\"Average cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Visualize decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['No Irrigation Needed', 'Irrigation Needed'],\n",
    "    filled=True,\n",
    "    rounded=True\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix visualization (fixed)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix as a heatmap (corrected)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Irrigation Needed', 'Irrigation Needed'],\n",
    "            yticklabels=['No Irrigation Needed', 'Irrigation Needed'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Splitting Data for Naive Bayes (keep this separate)\n",
    "X_bayes = df.drop(columns=['Irrigation'])\n",
    "y_bayes = df['Irrigation']\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_bayes_train, X_bayes_test, y_bayes_train, y_bayes_test = train_test_split(X_bayes, y_bayes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_bayes_train, y_bayes_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_bayes_pred = gnb.predict(X_bayes_test)\n",
    "bayes_accuracy = accuracy_score(y_bayes_test, y_bayes_pred)\n",
    "print(f\"\\nNaive Bayes Accuracy: {bayes_accuracy:.4f}\")\n",
    "print(classification_report(y_bayes_test, y_bayes_pred))\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(gnb, X_bayes, y_bayes, cv=5)  # cv=5 means using 5 folds\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame to compare predictions and actual values\n",
    "predictions = pd.DataFrame({\n",
    "    'Actual': y_bayes_test,\n",
    "    'Predicted': y_bayes_pred\n",
    "})\n",
    "\n",
    "# Count the number of mismatched data\n",
    "mismatches = (predictions['Actual'] != predictions['Predicted']).sum()\n",
    "\n",
    "# Display prediction table\n",
    "print(predictions.head(10))  # Display the first 10 rows\n",
    "print(f\"\\nNumber of mismatched data in the entire dataset: {mismatches}\")\n",
    "\n",
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_bayes_test, y_bayes_pred)\n",
    "\n",
    "# Display confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Irrigation Needed', 'Irrigation Needed'],\n",
    "            yticklabels=['No Irrigation Needed', 'Irrigation Needed'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Add additional machine learning algorithms for comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ADDITIONAL CLASSIFICATION MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the same train-test split for fair comparison\n",
    "X_comp = df.drop(columns=['Irrigation'])\n",
    "y_comp = df['Irrigation']\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(X_comp, y_comp, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling for better performance on some algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_comp)\n",
    "X_test_scaled = scaler.transform(X_test_comp)\n",
    "\n",
    "# 1. Support Vector Machine (SVM)\n",
    "from sklearn.svm import SVC\n",
    "print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train_comp)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test_comp, svm_pred)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "print(classification_report(y_test_comp, svm_pred))\n",
    "\n",
    "# 2. Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=8, min_samples_split=5,\n",
    "                                 class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train_comp, y_train_comp)\n",
    "rf_pred = rf_model.predict(X_test_comp)\n",
    "rf_accuracy = accuracy_score(y_test_comp, rf_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(classification_report(y_test_comp, rf_pred))\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_comp.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# 3. Gradient Boosting (Improved)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"\\n--- Gradient Boosting (Improved) ---\")\n",
    "\n",
    "# Grid search for optimal parameters\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Use a smaller grid for efficiency while maintaining performance improvements\n",
    "gb_param_grid_small = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [0.8]\n",
    "}\n",
    "\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_param_grid_small,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train_scaled, y_train_comp)  # Using scaled data for better performance\n",
    "print(f\"Best Gradient Boosting parameters: {gb_grid.best_params_}\")\n",
    "\n",
    "# Use the best model found\n",
    "gb_best = gb_grid.best_estimator_\n",
    "gb_pred_best = gb_best.predict(X_test_scaled)\n",
    "gb_accuracy_best = accuracy_score(y_test_comp, gb_pred_best)\n",
    "print(f\"Improved Gradient Boosting Accuracy: {gb_accuracy_best:.4f}\")\n",
    "print(classification_report(y_test_comp, gb_pred_best))\n",
    "\n",
    "# Save the improved model accuracy for comparison\n",
    "gb_accuracy = gb_accuracy_best\n",
    "\n",
    "# 4. K-Nearest Neighbors (Improved)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"\\n--- K-Nearest Neighbors (Improved) ---\")\n",
    "\n",
    "# Function to find optimal K value\n",
    "def find_optimal_k(X_train, y_train, X_test, y_test, max_k=30):\n",
    "    k_values = list(range(1, max_k + 1))\n",
    "    accuracies = []\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='distance')  # Using distance weighting\n",
    "        knn.fit(X_train, y_train)\n",
    "        accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Find best K\n",
    "    best_k = k_values[accuracies.index(max(accuracies))]\n",
    "    return best_k, max(accuracies), k_values, accuracies\n",
    "\n",
    "# Find optimal K value\n",
    "best_k, best_accuracy, k_values, accuracies = find_optimal_k(X_train_scaled, y_train_comp, X_test_scaled, y_test_comp)\n",
    "print(f\"Optimal K value: {best_k}\")\n",
    "\n",
    "# Visualize K value vs. accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'o-')\n",
    "plt.xlabel('K Value (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Value vs. Accuracy for KNN')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# More comprehensive KNN parameter tuning\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [best_k-2, best_k-1, best_k, best_k+1, best_k+2] if best_k > 2 else [1, 2, 3, 4, 5],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'p': [1, 2]  # p=1 for manhattan, p=2 for euclidean\n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    knn_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "knn_grid.fit(X_train_scaled, y_train_comp)\n",
    "print(f\"Best KNN parameters: {knn_grid.best_params_}\")\n",
    "\n",
    "# Use the best model found\n",
    "knn_best = knn_grid.best_estimator_\n",
    "knn_pred_best = knn_best.predict(X_test_scaled)\n",
    "knn_accuracy_best = accuracy_score(y_test_comp, knn_pred_best)\n",
    "print(f\"Improved KNN Accuracy: {knn_accuracy_best:.4f}\")\n",
    "print(classification_report(y_test_comp, knn_pred_best))\n",
    "\n",
    "# Save the improved model accuracy for comparison\n",
    "knn_accuracy = knn_accuracy_best\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "models = {\n",
    "    'Decision Tree': test_accuracy,\n",
    "    'Naive Bayes': bayes_accuracy,\n",
    "    'SVM': svm_accuracy,\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'Gradient Boosting': gb_accuracy,\n",
    "    'KNN': knn_accuracy\n",
    "}\n",
    "\n",
    "# Sort models by accuracy\n",
    "sorted_models = dict(sorted(models.items(), key=lambda item: item[1], reverse=True))\n",
    "for model_name, accuracy in sorted_models.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_models.keys(), sorted_models.values(), color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim(0.6, 1.0)  # Setting y-axis limits for better visualization\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for i, (model, acc) in enumerate(sorted_models.items()):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update models dictionary with improved values\n",
    "models = {\n",
    "    'Decision Tree': test_accuracy,\n",
    "    'Naive Bayes': bayes_accuracy,\n",
    "    'SVM': svm_accuracy,\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'Gradient Boosting': gb_accuracy,\n",
    "    'KNN': knn_accuracy\n",
    "}\n",
    "\n",
    "# Sort models by accuracy\n",
    "sorted_models = dict(sorted(models.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"UPDATED MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for model_name, accuracy in sorted_models.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize updated model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_models.keys(), sorted_models.values(), color='skyblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Updated Model Comparison')\n",
    "plt.ylim(0.6, 1.0)  # Setting y-axis limits for better visualization\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for i, (model, acc) in enumerate(sorted_models.items()):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the best KNN model for future use\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a directory for models if it doesn't exist\n",
    "model_dir = r\"/content/drive/MyDrive/KNNModel\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the KNN model using joblib (faster and better for larger models)\n",
    "joblib_path = os.path.join(model_dir, \"knn_irrigation_model.joblib\")\n",
    "joblib.dump(knn_best, joblib_path)\n",
    "print(f\"\\nModel saved to: {joblib_path}\")\n",
    "\n",
    "# Also save the scaler for preprocessing new data\n",
    "scaler_path = os.path.join(model_dir, \"knn_scaler.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names = {\n",
    "    'features': list(X_comp.columns)\n",
    "}\n",
    "with open(os.path.join(model_dir, \"feature_names.pkl\"), 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "\n",
    "# Example of how to load and use the model\n",
    "print(\"\\nExample code to load and use the model:\")\n",
    "print(\"\"\"\n",
    "# Load KNN model\n",
    "import joblib\n",
    "model = joblib.load('knn_irrigation_model.joblib')\n",
    "scaler = joblib.load('knn_scaler.joblib')\n",
    "\n",
    "# Prepare input data (must have same features as training data)\n",
    "# Example input: [CropType, SoilMoisture, temperature, Humidity]\n",
    "input_data = [[3, 500, 28, 65]]  # Example values\n",
    "\n",
    "# Scale the input data\n",
    "scaled_input = scaler.transform(input_data)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(scaled_input)\n",
    "print(f\"Irrigation needed: {'Yes' if prediction[0] == 1 else 'No'}\")\n",
    "\"\"\")\n",
    "\n",
    "# Code for Google Colab and GCP storage\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GOOGLE COLAB AND GCP INTEGRATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"# Code to use in Google Colab to save model to GCP:\")\n",
    "print(\"\"\"\n",
    "# 1. Install required packages in Colab\n",
    "!pip install google-cloud-storage\n",
    "\n",
    "# 2. Upload your GCP service account key to Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload your GCP service account JSON key file\n",
    "\n",
    "# 3. Set GCP authentication\n",
    "import os\n",
    "key_path = next(iter(uploaded.keys()))  # Get the filename of the uploaded key\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n",
    "\n",
    "# 4. Save your model in Colab\n",
    "import joblib\n",
    "# First train your model or load the existing one\n",
    "# knn_model = ...\n",
    "\n",
    "# Save locally in Colab\n",
    "joblib.dump(knn_model, 'knn_irrigation_model.joblib')\n",
    "joblib.dump(scaler, 'knn_scaler.joblib')\n",
    "\n",
    "# 5. Upload to GCP bucket\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcp_bucket(bucket_name, source_file_name, destination_blob_name):\n",
    "    '''Uploads a file to the GCP bucket'''\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}\")\n",
    "\n",
    "# Upload model files to GCP bucket\n",
    "bucket_name = 'your-bucket-name'  # Replace with your actual bucket name\n",
    "upload_to_gcp_bucket(bucket_name, 'knn_irrigation_model.joblib', 'models/knn_irrigation_model.joblib')\n",
    "upload_to_gcp_bucket(bucket_name, 'knn_scaler.joblib', 'models/knn_scaler.joblib')\n",
    "\n",
    "# 6. To later load the model from GCP in another Colab notebook or application:\n",
    "def download_from_gcp_bucket(bucket_name, source_blob_name, destination_file_name):\n",
    "    '''Downloads a file from the GCP bucket'''\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"File {source_blob_name} downloaded to {destination_file_name}\")\n",
    "\n",
    "# Download model files from GCP bucket\n",
    "download_from_gcp_bucket(bucket_name, 'models/knn_irrigation_model.joblib', 'knn_irrigation_model.joblib')\n",
    "download_from_gcp_bucket(bucket_name, 'models/knn_scaler.joblib', 'knn_scaler.joblib')\n",
    "\n",
    "# Load the downloaded model\n",
    "model = joblib.load('knn_irrigation_model.joblib')\n",
    "scaler = joblib.load('knn_scaler.joblib')\n",
    "\"\"\")\n",
    "\n",
    "# Add code to create a simple prediction function\n",
    "print(\"\\n# Simple prediction function example:\")\n",
    "print(\"\"\"\n",
    "def predict_irrigation_needed(model, scaler, crop_type, soil_moisture, temperature, humidity):\n",
    "    '''\n",
    "    Makes a prediction using the trained KNN model\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained KNN model\n",
    "    - scaler: Fitted StandardScaler\n",
    "    - crop_type: Encoded crop type (0-8)\n",
    "    - soil_moisture: Soil moisture value\n",
    "    - temperature: Temperature value\n",
    "    - humidity: Humidity value\n",
    "\n",
    "    Returns:\n",
    "    - Boolean indicating if irrigation is needed\n",
    "    '''\n",
    "    # Prepare input data\n",
    "    input_data = [[crop_type, soil_moisture, temperature, humidity]]\n",
    "\n",
    "    # Scale the input data\n",
    "    scaled_input = scaler.transform(input_data)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(scaled_input)[0]\n",
    "\n",
    "    return bool(prediction)\n",
    "\n",
    "# Example usage\n",
    "needs_irrigation = predict_irrigation_needed(model, scaler,\n",
    "                                           crop_type=3,      # Maize\n",
    "                                           soil_moisture=300,\n",
    "                                           temperature=30,\n",
    "                                           humidity=40)\n",
    "print(f\"Irrigation needed: {'Yes' if needs_irrigation else 'No'}\")\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39213913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local Loading\n",
    "print(\"\\nExample code to load and use the model:\")\n",
    "import joblib\n",
    "model = joblib.load('/content/drive/MyDrive/KNNModel/knn_irrigation_model.joblib')\n",
    "scaler = joblib.load('/content/drive/MyDrive/KNNModel/knn_scaler.joblib')\n",
    "\n",
    "# Prepare input data (must have same features as training data)\n",
    "# Example input: [CropType, SoilMoisture, temperature, Humidity]\n",
    "input_data = [[3, 500, 28, 65]]  # Example values\n",
    "\n",
    "# Scale the input data\n",
    "scaled_input = scaler.transform(input_data)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(scaled_input)\n",
    "print(f\"Irrigation needed: {'Yes' if prediction[0] == 1 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebase manual data entry\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Firebase if not already initialized\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(os.getenv('FIREBASE_CREDENTIALS_PATH'))\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        'databaseURL': os.getenv('FIREBASE_DATABASE_URL')\n",
    "    })\n",
    "\n",
    "# Crop type mapping dictionary (name to encoded value)\n",
    "crop_mapping = {\n",
    "    \"Coffee\": 0,\n",
    "    \"Garden Flowers\": 1,\n",
    "    \"Groundnuts\": 2,\n",
    "    \"Maize\": 3,\n",
    "    \"Paddy\": 4,\n",
    "    \"Potato\": 5,\n",
    "    \"Pulse\": 6,\n",
    "    \"Sugarcane\": 7,\n",
    "    \"Wheat\": 8\n",
    "}\n",
    "\n",
    "# Function to convert crop name to number\n",
    "def get_crop_code(crop_name):\n",
    "    if isinstance(crop_name, int) and 0 <= crop_name <= 8:\n",
    "        return crop_name  # Already a valid code\n",
    "    elif crop_name in crop_mapping:\n",
    "        return crop_mapping[crop_name]\n",
    "    else:\n",
    "        print(f\"Warning: '{crop_name}' not found in crop mapping. Using default (Maize:3)\")\n",
    "        return 3  # Default to Maize if not found\n",
    "\n",
    "# Get crop input from user\n",
    "print(\"Available crops:\", \", \".join(crop_mapping.keys()))\n",
    "crop_input = input(\"Enter crop name (or press Enter for 'Groundnuts'): \").strip()\n",
    "if not crop_input:\n",
    "    crop_input = \"Groundnuts\"  # Default crop\n",
    "\n",
    "# Get other sensor values\n",
    "try:\n",
    "    temp = float(input(\"Enter temperature (Â°C) [28.5]: \") or \"28.5\")\n",
    "    humidity = float(input(\"Enter humidity (%) [65.3]: \") or \"65.3\")\n",
    "    soil_moisture = float(input(\"Enter soil moisture (%) [35.2]: \") or \"35.2\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Using default values.\")\n",
    "    temp = 28.5\n",
    "    humidity = 65.3\n",
    "    soil_moisture = 35.2\n",
    "\n",
    "# Convert crop name to number\n",
    "crop_code = get_crop_code(crop_input)\n",
    "\n",
    "# Create test data\n",
    "test_data = {\n",
    "    \"CropType\": crop_code,\n",
    "    \"temperature\": temp,\n",
    "    \"Humidity\": humidity,\n",
    "    \"SoilMoisture\": soil_moisture\n",
    "}\n",
    "\n",
    "# Print what we're sending to Firebase\n",
    "print(\"\\nSending to Firebase:\")\n",
    "print(f\"  Crop: {crop_input} (code: {crop_code})\")\n",
    "print(f\"  Temperature: {temp}Â°C\")\n",
    "print(f\"  Humidity: {humidity}%\")\n",
    "print(f\"  Soil Moisture: {soil_moisture}%\")\n",
    "\n",
    "# Send data to Firebase\n",
    "sensor_ref = db.reference(\"sensor\")\n",
    "sensor_ref.set(test_data)\n",
    "\n",
    "# Store sensor data permanently in Realtime Database (instead of Firestore)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "record = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"sensor_data\": test_data,\n",
    "    \"crop_name\": crop_input,\n",
    "    \"crop_code\": crop_code\n",
    "}\n",
    "\n",
    "# Add to the \"history/sensors\" node in Realtime Database\n",
    "history_ref = db.reference(\"history/sensors\")\n",
    "history_ref.child(timestamp).set(record)\n",
    "\n",
    "print(\"\\nTest data added to Firebase successfully!\")\n",
    "print(f\"Data permanently stored in database with ID: {timestamp}\")\n",
    "print(\"Run 1stcode.py to get the irrigation prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebase prediction\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "import joblib, pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Only initialize if not already initialized\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(os.getenv('FIREBASE_CREDENTIALS_PATH'))\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        'databaseURL': os.getenv('FIREBASE_DATABASE_URL')\n",
    "    })\n",
    "\n",
    "# Load model, scaler, feature names\n",
    "model = joblib.load(os.getenv('MODEL_PATH'))\n",
    "scaler = joblib.load(os.getenv('SCALER_PATH'))\n",
    "with open(os.getenv('FEATURE_NAMES_PATH'), \"rb\") as f:\n",
    "    feature_names_dict = pickle.load(f)\n",
    "\n",
    "# Extract the list of feature names\n",
    "feature_list = feature_names_dict['features']\n",
    "print(f\"Required features: {feature_list}\")\n",
    "\n",
    "# Get sensor data from Firebase\n",
    "sensor_data = db.reference(\"sensor\").get()\n",
    "if not sensor_data:\n",
    "    print(\"No sensor data found.\")\n",
    "else:\n",
    "    print(\"Raw sensor data from Firebase:\", sensor_data)\n",
    "\n",
    "    # Create a properly formatted input array with all required features\n",
    "    input_data = []\n",
    "    for feature in feature_list:\n",
    "        if feature in sensor_data:\n",
    "            input_data.append(sensor_data[feature])\n",
    "        else:\n",
    "            print(f\"Warning: Feature '{feature}' not found in sensor data. Using default value 0.\")\n",
    "            input_data.append(0)\n",
    "\n",
    "    print(f\"Processed input data: {input_data}\")\n",
    "\n",
    "    # Verify we have the correct number of features\n",
    "    if len(input_data) != len(feature_list):\n",
    "        print(f\"Error: Input data has {len(input_data)} features, but model expects {len(feature_list)} features.\")\n",
    "    else:\n",
    "        # Create a DataFrame with feature names to avoid the warning\n",
    "        import pandas as pd\n",
    "        input_df = pd.DataFrame([input_data], columns=feature_list)\n",
    "\n",
    "        # Predict\n",
    "        scaled = scaler.transform(input_df)\n",
    "        prediction = int(model.predict(scaled)[0])\n",
    "\n",
    "        # Push prediction back to Firebase\n",
    "        db.reference(\"prediction\").set(prediction)\n",
    "        print(f\"Prediction sent to Firebase: {prediction} ({'Irrigation needed' if prediction == 1 else 'No irrigation needed'})\")\n",
    "\n",
    "        # Store data permanently in Realtime Database instead of Firestore\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        # Create data record with all relevant information\n",
    "        record = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"sensor_data\": sensor_data,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_text\": 'Irrigation needed' if prediction == 1 else 'No irrigation needed'\n",
    "        }\n",
    "\n",
    "        # Add to the \"history/predictions\" node in Realtime Database\n",
    "        history_ref = db.reference(\"history/predictions\")\n",
    "        history_ref.child(timestamp).set(record)\n",
    "        print(f\"Data permanently stored in database with ID: {timestamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Servo motor ON/OFF\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, db\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Firebase (if not already initialized)\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(os.getenv('FIREBASE_CREDENTIALS_PATH'))\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        'databaseURL': os.getenv('FIREBASE_DATABASE_URL')\n",
    "    })\n",
    "\n",
    "def simulate_hardware():\n",
    "    print(\"=== Smart Irrigation Hardware Simulator ===\")\n",
    "    print(\"Starting simulation...\")\n",
    "\n",
    "    # Create reference to the prediction node\n",
    "    prediction_ref = db.reference(\"prediction\")\n",
    "\n",
    "    # Track servo state\n",
    "    servo_on = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Get current prediction value\n",
    "            prediction = prediction_ref.get()\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            print(f\"\\n[{current_time}] Checking prediction value...\")\n",
    "            print(f\"Current prediction from Firebase: {prediction}\")\n",
    "\n",
    "            # Logic to determine if servo should be on/off\n",
    "            # Assuming prediction = 1 means \"turn on irrigation\"\n",
    "            if prediction == 1 and not servo_on:\n",
    "                servo_on = True\n",
    "                print(\"ðŸ”„ ACTION: Servo motor activated - IRRIGATION STARTED\")\n",
    "                # In real hardware, this would trigger GPIO pins to control the servo\n",
    "            elif prediction == 0 and servo_on:\n",
    "                servo_on = False\n",
    "                print(\"ðŸ”„ ACTION: Servo motor deactivated - IRRIGATION STOPPED\")\n",
    "            else:\n",
    "                print(f\"ðŸ”„ Servo status: {'ON' if servo_on else 'OFF'} (No change needed)\")\n",
    "\n",
    "            # Hardware simulator status\n",
    "            print(f\"ðŸ’§ Irrigation system status: {'ACTIVE' if servo_on else 'INACTIVE'}\")\n",
    "\n",
    "            # Wait before checking again\n",
    "            print(\"Waiting 5 seconds before next check...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSimulation stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in simulation: {e}\")\n",
    "    finally:\n",
    "        print(\"Simulation ended\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulate_hardware()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c615609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "with open(os.getenv('FEATURE_NAMES_PATH'), \"rb\") as f:\n",
    "    feature_names = pickle.load(f)\n",
    "print(feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
